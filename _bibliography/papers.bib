---
---


@inproceedings{park2025expertsenoughoptimalsemantic,
      title={How Many Experts Are Enough? Towards Optimal Semantic Specialization for Mixture-of-Experts}, 
      author={Sumin Park and Noseong Park},
      abbr={AAAI},
      year={2025},
      arxiv={2512.19765},
      booktitle={Association for the Advancement of Artificial Intelligence (AAAI)},
      archivePrefix={arXiv},
      preview={mass.png},
      primaryClass={cs.LG},
      pdf={https://arxiv.org/pdf/2512.19765},
      abstract={Finding the optimal configuration of Sparse Mixture-ofExperts (SMoE) that maximizes semantic differentiation among experts is essential for exploiting the full potential of MoE architectures. However, existing SMoE frameworks either heavily rely on hyperparameter tuning or overlook the importance of diversifying semantic roles across experts when adapting the expert pool size. We propose Mixture-of-Experts for Adaptive Semantic Specialization (MASS), a semanticaware MoE framework for adaptive expert expansion and dynamic routing. MASS introduces two key advancements: (i) a gradient-based semantic drift detector that prompts targeted expert expansion when the existing expert pool lacks capacity to capture the full semantic diversity of the data, and (ii) an integration of adaptive routing strategy that dynamically adjusts expert usage based on token-level routing confidence mass. We first demonstrate that MASS reliably converges to the point of optimal balance between cost-performance trade-off with notably improved sematic specialization in a highly controlled synthetic setup. Further empirical results on real-world datasets across language and vision domains show that MASS consistently outperforms a range of strong MoE baselines, demonstrating its domain robustness and enhanced expert specialization.},
      url={https://arxiv.org/abs/2512.19765}, 
      selected=true
}

@inproceedings{parkdars,
      title={DARS: ROBUST SPARSE FINE-TUNING WITH REGULARIZED SUBSPACE DISALIGNMENT},
      author={Park, Sumin and Park, Noseong},
      abbr={ICLR Workshop},
      arkiv={},
      preview={dars.png},
      booktitle={First Workshop on Scalable Optimization for Efficient and Adaptive Foundation Models (SCOPE)},
      abstract={Recent works have identified the alignment, which measures a layerwise weight correlation, as a novel yet crucial mechanism for feature learning. We investigate an underlying connection between the alignment learning and the structural fitting of a network to the training data span. Based on this insight, we further demonstrate that fine-tuning on out-of-distribution (OOD) data disrupts this well-aligned structure fitted during the pre-training phase, degrading generalization performance. To address this, we propose DARS, DisAlignment-Regularized Sparse fine-tuning, a novel sparse fine-tuning approach that mitigates disalignment by letting the gradient update to be partially constrained within the principal subspace of the pre-trained network, constructed based on the in-distribution (ID) data used for its pre-training. Specifically, we define the two disjoint subsets of trainable parameters for sparse channel unfreezing: i) a random subset and ii) a subset with higher gradient projections onto the principal subspace. The latter serves as a disalignment regularizer during fine-tuning, while the random subset ensures a minimal bias in parameter selection. By adjusting the ratio between the two subsets, we can control the strength of subspace regularization, thereby balancing the trade-off between generalization capacity and strong fitting to new downstream tasks. By employing DARS, we achieved SOTA performance on various benchmarks, including commonsense and arithmetic reasoning tasks, across LLaMA-7B and LLaMA2-7B.},
      pdf={https://openreview.net/pdf?id=nV6e9KjJrH},
      url={https://openreview.net/forum?id=nV6e9KjJrH&referrer=%5Bthe%20profile%20of%20Sumin%20Park%5D(%2Fprofile%3Fid%3D~Sumin_Park2)}
}

@inproceedings{choi2024pandaexpandedwidthawaremessage,
      title={PANDA: Expanded Width-Aware Message Passing Beyond Rewiring}, 
      author={Jeongwhan Choi and Sumin Park and Hyowon Wi and Sung-Bae Cho and Noseong Park},
      year={2024},
      abbr={ICML},
      arxiv={2406.03671},
      booktitle={International Conference on Machine Learning (ICML)},
      preview={panda.png},
      archivePrefix={arXiv},
      abstract={Recent research in the field of graph neural network (GNN) has identified a critical issue known as "over-squashing," resulting from the bottleneck phenomenon in graph structures, which impedes the propagation of long-range information. Prior works have proposed a variety of graph rewiring concepts that aim at optimizing the spatial or spectral properties of graphs to promote the signal propagation. However, such approaches inevitably deteriorate the original graph topology, which may lead to a distortion of information flow. To address this, we introduce an expanded width-aware (PANDA) message passing, a new message passing paradigm where nodes with high centrality, a potential source of over-squashing, are selectively expanded in width to encapsulate the growing influx of signals from distant nodes. Experimental results show that our method outperforms existing rewiring methods, suggesting that selectively expanding the hidden state of nodes can be a compelling alternative to graph rewiring for addressing the over-squashing.},
      primaryClass={cs.LG},
      pdf={https://arxiv.org/pdf/2406.03671},
      url={https://arxiv.org/abs/2406.03671}, 
      selected=true
}
